\documentclass{amsart}
\usepackage{amsmath,amssymb,amsthm,framed}

\renewcommand{\emptyset}{\varnothing}
\renewcommand{\phi}{\varphi}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{conjecture}[proposition]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}

\newtheoremstyle{problem}{}{}{}{}
{\bfseries}{} % Note that final punctuation is omitted.
{\newline}{}

\theoremstyle{problem}
\newtheorem{problem}[proposition]{Problem}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\title{The problem of the randomly walked dog}
\author{Scallop Ye}

\begin{document}

\begin{abstract}
    In this article, we present a novel problem in the spirit of
    the well-known problem of the random walk \cite{pearson},
    along with a curious conjecture on the property of its solutions.
\end{abstract}

\maketitle

\section{Introduction}

A man walks the dog every day. The dog will prepare $n\ge3$ cards beforehand,
each containing a direction and a positive distance.
They start from home and repeat the following process $n$ times:
(1) choose a random card; (2) walk the given distance in the given direction; (3) throw the card away.

After these $n$ rounds, they shall be exactly back home. The dog wins a treat
if they visited home exactly twice and any other place at most once.
Now, help the dog to find $n$ cards that maximize the probability
of winning the treat in a walk.

\begin{remark}
    This maximum probability is later conjectured to equal
    $2/(n-1)$, despite the lack of a general solution to the problem.
\end{remark}

\section{Definitions}

We now proceed to formally define the problem.
The cards the dog prepares should represent
a sequence of nonzero vectors that sum to zero.

\begin{definition}
    A \emph{step vector} is a nonzero vector in $\mathbb{R}^2$.
    An \emph{$n$-step sequence} $S$, or generally a \emph{step sequence},
    is a finite sequence $(s_1,s_2,\dots,s_n)$ of step vectors;
    we say that $S$ is \emph{zero-sum} iff $\sum_{i=1}^{n}s_i=(0,0)$.
\end{definition}

To study the points the pair visits in a walk,
we shall introduce polygonal paths.

\begin{definition}
    A \emph{polygonal path} is a finite sequence $(p_0,p_1,\dots,p_n)$
    of points called its \emph{vertices}; the line segments
    $\overline{p_0p_1},\overline{p_1p_2},\dots,\overline{p_{n-1}p_n}$
    are called its \emph{edges}.
\end{definition}

The winning conditions for the dog naturally translate to whether the polygonal
path formed in a walk does not ``intersect itself'',
or, more precisely, whether the path is simple.

\begin{definition}
    A polygonal path $(p_0,p_1,\dots,p_n)$ is \emph{simple}
    iff for all $0<i\le j<n$,
    \[
        \overline{p_{i-1}p_i}\cap\overline{p_jp_{j+1}}=
        \begin{cases}
            \{p_i\}            & \text{if $i=j$},                       \\
            \{p_0\}\cap\{p_n\} & \text{if $n\ge3$ and $(i,j)=(1,n-1)$}, \\
            \emptyset          & \text{otherwise}.
        \end{cases}
    \]
\end{definition}

Geometrically, a simple path is one in which only consecutive
edges intersect and only at their endpoints; it may also
be \emph{closed} as specially dealt with in the second case above.
Next, we define a way to create a path from a step sequence.

\begin{definition}
    Let $S:=(s_1,s_2,\dots,s_n)$ be a step sequence. The \emph{walk} of $S$
    is the polygonal path $(p_0,p_1,\dots,p_n)$
    where $p_0=(0,0)$ and $p_i=p_{i-1}+s_i$ for $1\le i\le n$.
\end{definition}

Finally, we define the value we are maximizing and then the problem.

\begin{definition}
    The \emph{simplicity} of a step sequence $S$
    is the number of permutations $S'$ of $S$
    such that the walk of $S'$ is simple.
\end{definition}

\begin{problem}[\textsc{The Problem of the Randomly Walked Dog}]
\label{prob:rwd}
\begin{framed}
    \begin{tabular}{@{}ll}
        \textit{Instance:} &
        A natural number $n\ge3$. \\

        \textit{Task:}     &
        Find a zero-sum $n$-step sequence of maximum simplicity.
    \end{tabular}
    \vskip -2pt
\end{framed}
\end{problem}

This could be viewed as a combinatorial optimization problem,
because w.l.o.g. we might restrict step vectors
to those in $\mathbb{Z}^2$ of magnitude less than some $f(n)$,
hence a finite set of feasible solutions. However, this problem is
quite peculiar in that
(a) each instance is only a single natural number,
(b) the set of feasible solutions is not readily restricted to be finite,
and (c) the value of a feasible solution cannot even be computed
in reasonable time for $n$ moderately large.

Despite all these peculiarities, the solutions to the problem seem
to have the nice property that their values can be expressed
in a simple closed form (see Conjecture \ref{con:value_formula}).
I thus claim that there is a polynomial-time algorithm for the problem,
which I invite the reader to prove or disprove.

\section{Preliminary Results}

Let us fix $n\ge3$. We first show that it is always possible for the dog to win.

\begin{proposition}
    Let $S$ be a zero-sum $n$-step sequence. If the step vectors in $S$ are not all collinear,
    then there exists a permutation $S'$ of $S$ such that the walk of $S'$ is simple.
\end{proposition}

\begin{proof}
    Let $S':=(s_1,s_2,\dots,s_n)$ be a permutation of $S$ such that, with
    $\theta_i$ being the argument of $s_i$ satisfying $-\pi<\theta_i\le\pi$,
    the sequence $(\theta_1,\theta_2,\dots,\theta_n)$ is increasing.
    Since $S$ is zero-sum, $S'$ is also zero-sum.
    Let $s_{n+1}:=s_1$ and denote by $\alpha_i$ the angle between
    $s_{i+1}$ and $s_i$ satisfying $0\le\alpha_i\le\pi$.
    If $\theta_n-\theta_1<\pi$, then $S'$ would not be zero-sum.
    Thus we have $\alpha_n=2\pi-(\theta_n-\theta_1)$.
    If there were some $1\le i<n$ such that $\theta_{i+1}-\theta_{i}>\pi$,
    then $S'$ would again not be zero-sum. So we have $\alpha_i=\theta_{i+1}-\theta_i$
    for all $1\le i<n$ and that $\sum_{i=1}^n\alpha_i=2\pi$.
    Let $P$ be the walk of $S'$, a polygon of total absolute curvature $2\pi$.
    By Fenchel's theorem (generalized to any closed curve by Milnor
    \cite[Theorem 3.4]{milnor}), we know that $P$ is convex. Since $P$
    does not lie on a line, it follows from \cite[Theorem 2.2]{ye}
    that $P$ is simple.
\end{proof}

By computer simulation on Problem \ref{prob:rwd}, we observe that the maximum
simplicities on record follow a very simple formula, hence the following conjecture.

\begin{conjecture}
    \label{con:value_formula}
    The maximum simplicity of a zero-sum $n$-step sequence
    equals \[2\cdot n!/(n-1).\]
\end{conjecture}

\begin{thebibliography}{9}

    \bibitem{milnor}
    J. W. Milnor,
    \emph{On the total curvature of knots},
    Ann. of Math.
    \textbf{52} (1950), no. 2, 248--257.

    \bibitem{pearson}
    K. Pearson,
    \emph{The problem of the random walk},
    Nature \textbf{72} (1905), no. 1865, 294.

    \bibitem{ye}
    S. Ye,
    \emph{A convex closed curve is simple iff it does not lie on a line},
    preprint.

\end{thebibliography}

\end{document}