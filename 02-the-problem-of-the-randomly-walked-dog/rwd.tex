\documentclass{amsart}
\usepackage{amsmath,amssymb,amsthm,framed}

\renewcommand{\emptyset}{\varnothing}
\renewcommand{\phi}{\varphi}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{conjecture}[proposition]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}

\newtheoremstyle{problem}{}{}{}{}
{\bfseries}{} % Note that final punctuation is omitted.
{\newline}{}

\theoremstyle{problem}
\newtheorem{problem}[proposition]{Problem}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\title{The Problem of the Randomly Walked Dog}
\author{Scallop Ye}

\begin{document}

\begin{abstract}
    In 1905, Karl Pearson \cite{pearson} proposed on \emph{Nature} the
    famous problem of the random walk. In this article, we present a novel
    combinatorial optimization problem in the spirit of Pearson's one,
    along with a curious conjecture on the property of its solutions.
\end{abstract}

\maketitle

\section{Introduction}

A man walks the dog every day. The dog will prepare $n$ cards beforehand ($n\ge3$),
each containing a direction and a distance.
They start from home and repeat the following process $n$ times:

\begin{enumerate}
    \item choose a card randomly;
    \item turn to the direction given on the card;
    \item walk the distance given on the card in a straight line;
    \item throw the card away.
\end{enumerate}

After these $n$ rounds, they shall be exactly back home. The dog wins a treat
if they visited home exactly twice and any other place at most once.

Now, you are to instruct the dog how to maximize the probability of winning the
treat in a walk. Note that too much calculation exhausts the dog.

\begin{remark}
    As one might have seen, this problem differs from Pearsonâ€™s one in that
    it is not inherently probabilistic. We will be using permutations
    instead to formally define the problem in the next section.
    It is later conjectured that the maximum probability required is exactly
    $2/(n-1)$, despite the lack of a general solution.
\end{remark}

\section{Definitions}

Let us take a look at the cards first. The cards the dog prepares should represent
a sequence of nonzero vectors that sum to zero. We may then define

\begin{definition}
    A \emph{step vector} is a nonzero vector in $\mathbb{R}^2$.
    An \emph{$n$-step sequence} $S$, or generally a \emph{step sequence},
    is a finite sequence $(s_1,s_2,\dots,s_n)$ of step vectors;
    we say that $S$ is \emph{zero-sum} iff $\sum_{i=1}^{n}s_i=(0,0)$.
\end{definition}

Then in order to study the points the pair visits in a walk, we shall introduce
polygonal paths:

\begin{definition}
    A \emph{polygonal path} $P$ is a finite sequence $(p_0,p_1,\dots,p_n)$
    of points called its \emph{vertices}; the line segments
    $\overline{p_0p_1},\overline{p_1p_2},\dots,\overline{p_{n-1}p_n}$
    are called its \emph{edges}.
\end{definition}

The winning conditions for the dog naturally translate to whether the polygonal
path formed in a walk does not ``intersect itself'', or in other words whether
the path is simple.

\begin{definition}
    A polygonal path $(p_0,p_1,\dots,p_n)$ is \emph{simple}
    iff for all $0<i\le j<n$,
    \[
        \overline{p_{i-1}p_i}\cap\overline{p_jp_{j+1}}=
        \begin{cases}
            \{p_i\}            & \text{if $i=j$},                       \\
            \{p_0\}\cap\{p_n\} & \text{if $n\ge3$ and $(i,j)=(1,n-1)$}, \\
            \emptyset          & \text{otherwise}.
        \end{cases}
    \]
\end{definition}

Geometrically, a simple path is one in which only consecutive
edges intersect and only at their endpoints; it may also
be \emph{closed} as specially dealt with in the second case above.
Then, we define a way to create a path from a step sequence:

\begin{definition}
    Let $S=(s_1,s_2,\dots,s_n)$ be a step sequence. The \emph{walk} of $S$
    is the polygonal path $(p_0,p_1,\dots,p_n)$
    where $p_0=(0,0)$ and $p_i=p_{i-1}+s_i$ for $1\le i\le n$.
\end{definition}

Finally, we define the value we are maximizing and then the problem:

\begin{definition}
    The \emph{simplicity} of a step sequence $S$
    is the number of permutations $S'$ of $S$
    such that the walk of $S'$ is simple.
\end{definition}

\begin{problem}[\textsc{The Problem of the Randomly Walked Dog}]
\label{prob:rwd}
\begin{framed}
    \begin{tabular}{@{}ll}
        \textit{Instance:} &
        A natural number $n\ge3$. \\

        \textit{Task:}     &
        Find a zero-sum $n$-step sequence of maximum simplicity.
    \end{tabular}
    \vskip -2pt
\end{framed}
\end{problem}

This can be viewed as a combinatorial optimization problem,
because without loss of generality we may restrict step vectors
to those in $\mathbb{Z}^2$ with magnitudes less than some $f(n)$,
hence a finite set of feasible solutions. However, this problem is
quite peculiar in that
(a) each instance is only a single natural number,
(b) the set of feasible solutions is not easily restricted to be finite,
and (c) the value of a feasible solution is not easily computed
even for moderately large $n$.

Despite all these peculiarities, the solutions to the problem seem
to have such a nice property that their values are in
a simple closed form (see Conjecture \ref{conj:value_formula}).
One would naturally think that a clever algorithm ought to be found,
if only one that runs in linear time.

\section{Preliminary Results}

Let us fix $n\ge3$. We first present a proposition which
implies that it is always possible for the dog to win.


\begin{proposition}
    Let $S$ be a zero-sum $n$-step sequence. If the step vectors in $S$ are not all collinear,
    then there exists a permutation $S'$ of $S$ such that the walk of $S'$ is simple.
\end{proposition}

\begin{proof}
    Let $S'=(s_1,s_2,\dots,s_n)$ be a permutation of $S$ such that, with
    $\theta_i$ being the argument of $s_i$ satisfying $-\pi<\theta_i\le\pi$,
    the sequence $(\theta_1,\theta_2,\dots,\theta_n)$ is increasing.
    Since $S$ is zero-sum, $S'$ is also zero-sum.
    Let $s_{n+1}=s_1$ and denote by $\alpha_i$ the angle between
    $s_{i+1}$ and $s_i$ satisfying $0\le\alpha_i\le\pi$.
    If $\theta_n-\theta_1<\pi$, then $S'$ would not be zero-sum.
    Thus we have $\alpha_n=2\pi-(\theta_n-\theta_1)$.
    If there were some $1\le i<n$ such that $\theta_{i+1}-\theta_{i}>\pi$,
    then $S'$ would again not be zero-sum. So we have $\alpha_i=\theta_{i+1}-\theta_i$
    for all $1\le i<n$ and that $\sum_{i=1}^n\alpha_i=2\pi$.
    Let $P$ be the walk of $S'$, a polygon of total absolute curvature $2\pi$.
    By Fenchel's theorem (generalized to any closed curve by Milnor
    \cite[Theorem 3.4]{milnor}), we know that $P$ is convex.
    A convex polygon that does not lie entirely on a line is simple \cite{ye}.
\end{proof}

By computer simulation on Problem \ref{prob:rwd}, we observe that the maximum
simplicity on record follows a very simple formula, hence the following conjecture.

\begin{conjecture}
    \label{conj:value_formula}
    The maximum simplicity of a zero-sum $n$-step sequence
    is exactly $2\cdot n!/(n-1)$.
\end{conjecture}

\begin{thebibliography}{9}

    \bibitem{pearson}
    K. Pearson,
    \emph{The Problem of the Random Walk},
    Nature \textbf{72} (1905), 294.

    \bibitem{milnor}
    J. W. Milnor,
    \emph{On the Total Curvature of Knots},
    Ann. of Math.
    \textbf{52} (1950), no. 2, 248-257.

    \bibitem{ye}
    S. Ye,
    \emph{When Is a Convex Closed Curve Not Simple?},
    preprint.

\end{thebibliography}

\end{document}